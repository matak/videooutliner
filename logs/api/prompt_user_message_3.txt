Transcript:

=================
538
00:34:10,199 --> 00:34:12,039
Nespojene se kouknu, kde je tady čeština.

539
00:34:13,000 --> 00:34:14,380
Čeština 12.

540
00:34:16,199 --> 00:34:18,960
12 zpráv v celém datasetu.

541
00:34:18,960 --> 00:34:20,440
Kolik má ten dataset?

542
00:34:20,960 --> 00:34:25,599
129 000 řádek, takže teď to bylo 12 nebo 12 000.

543
00:34:27,820 --> 00:34:31,900
64 000, 28 000, takže opravdu 12 jenom zpráv je v češtině.

544
00:34:32,019 --> 00:34:37,739
Dobře no. Open Assistant I.O. vypadá, že nefunguje, to nevadí.

545
00:34:38,119 --> 00:34:40,420
Každopádně tady na tom jsem vám chtěl ukázat

546
00:34:40,420 --> 00:34:42,500
tu rozstříštěnost toho datasetu.

547
00:34:42,500 --> 00:34:44,420
Vidíte, že tady vlastně v tom datasetu,

548
00:34:44,579 --> 00:34:48,019
ať už to má ten train, má to 129 tisíc řádek,

549
00:34:48,019 --> 00:34:50,960
tak ten train dataset má prostě spoustu

550
00:34:51,079 --> 00:34:54,059
tady sloupečků, ze kterých si můžete vzít

551
00:34:54,260 --> 00:34:56,480
nějaký informace a vlastně si pak poskládat

552
00:34:56,559 --> 00:34:58,179
to, co z toho chcete trénovat.

553
00:34:58,739 --> 00:35:02,760
Takže jenom způsob vytvoření od toho datasetu

554
00:35:02,760 --> 00:35:04,239
je pro vás důležitý.

555
00:35:04,360 --> 00:35:08,159
Já bych vám radil, abyste používali

556
00:35:08,900 --> 00:35:13,460
tadyhletu strukturu. Tadyhleta struktura mi vůbec nedává smysl,

557
00:35:13,519 --> 00:35:16,059
protože to je jenom v případě, že byste opravdu chtěli pracovat

558
00:35:16,059 --> 00:35:19,000
jenom s tím jedním konkrétním modelem, nebo si byli jistý,

559
00:35:19,079 --> 00:35:24,400
že tadyhleten způsob formatování pokryje tu skupinu modelů,

560
00:35:24,400 --> 00:35:29,320
kterou vy plánujete použít. Mně to nedává teda smysl vůbec,

561
00:35:29,720 --> 00:35:33,179
takže já bych volil spíš jako takovejhle způsob zápisu.

562
00:35:33,179 --> 00:35:36,179
Můžete teda volit i ten vlastně...

563
00:35:36,880 --> 00:35:41,300
před tím ten format, což vlastně ukazuje tadyhle ten Open Assistant

564
00:35:42,119 --> 00:35:47,539
a z tohohle toho si pak vybrat ty sloupečky, které jsou pro vás hodný,

565
00:35:48,139 --> 00:35:52,539
text, ale záleží pak ještě na ty další, které tam do toho budete chtít dát,

566
00:35:52,579 --> 00:35:54,820
což by se dalo přirovnat něčemu takovýmhlemu.

567
00:35:56,059 --> 00:35:59,400
Říkám, oba ty formáty mi dávají smysl,

568
00:35:59,519 --> 00:36:01,599
já bych spíš inklinoval k něčemu takovému,

569
00:36:01,599 --> 00:36:07,039
kde už mám připravenou tu datovou sledu do té formy,

570
00:36:07,099 --> 00:36:10,679
kde z toho vidím, co vlastně ten model chci učit,

571
00:36:10,840 --> 00:36:14,380
protože z toho otázka-odpověď nebo input-output

572
00:36:14,380 --> 00:36:17,579
nebo user-assistant jsem schopnej odvodit,

573
00:36:17,699 --> 00:36:20,800
co tak asi ten dataset k čemu mi slouží.

574
00:36:20,880 --> 00:36:22,739
Toto ten dataset...

575
00:36:23,320 --> 00:36:25,900
úplná systém je takový víc obecnej,

576
00:36:25,940 --> 00:36:27,739
že z toho si pak můžete poskládat

577
00:36:27,739 --> 00:36:29,099
víc věcí.

578
00:36:30,000 --> 00:36:32,340
Budete ho učit, kolik se smazalo,

579
00:36:32,360 --> 00:36:33,500
nebo budete ho učit,

580
00:36:33,599 --> 00:36:34,860
jestli to, jestli

581
00:36:36,300 --> 00:36:37,779
toxicitudu, nebo

582
00:36:37,900 --> 00:36:39,579
prostě různý další,

583
00:36:39,619 --> 00:36:41,279
na základě těch dalších sloupečků,

584
00:36:41,679 --> 00:36:42,619
ho můžete učit

585
00:36:42,619 --> 00:36:44,579
jiný způsoby.

586
00:36:45,360 --> 00:36:46,720
Tak, to je dataset.

587
00:36:46,720 --> 00:36:48,940
Ukaž jim, můžu mít otázku k tomu datasetu.

588
00:36:49,320 --> 00:36:52,519
Většinou ten dataset se váže na nějaký konkrétní model,

589
00:36:52,539 --> 00:36:55,059
třeba v tomhle případě jsi říkal, že tam byl nějakej mistral,

590
00:36:55,059 --> 00:36:58,639
nebo něco takového, předtím, proč bych nepoužil rovnou jen ten model?

591
00:36:59,559 --> 00:37:09,039
A jak často se v praxi vyskytuje to, že potřebuji vzít dataset nějakého jiného modelu a dát si ho do, nevím, třeba Lamy místo Mistra?

592
00:37:09,220 --> 00:37:16,279
Dál mi je to spíš o tom, že vy si připravíte to, co chcete natrénovat, jakýkoliv model.

593
00:37:16,400 --> 00:37:20,440
Proto já říkám, že bych to spíš volil takovýhle nebo takovýhle format.

594
00:37:20,800 --> 00:37:26,619
A teď vy si můžete vybrat, že budete trénovat nebo dotrénovávat nebo fajntunovat.

595
00:37:26,579 --> 00:37:29,360
a zkusíte to na Mistralu použít.

596
00:37:29,400 --> 00:37:31,980
A v tu chvíli vlastně ten tokenizer vám vytvoří

597
00:37:32,000 --> 00:37:35,079
tenhle format, potom jsou vám stálo tokeny,

598
00:37:35,079 --> 00:37:38,360
které pak jdou do toho modelu a tam funguje celý ten proces

599
00:37:38,860 --> 00:37:40,179
toho fine tuningu.

600
00:37:40,420 --> 00:37:43,039
Ale pokud vy v tom datasetu budete mít

601
00:37:43,059 --> 00:37:45,320
jenom tenhle format, to znamená, že

602
00:37:45,539 --> 00:37:48,380
pokud bych chtěl to vyzkoušet na LAMě nebo na nějakém

603
00:37:48,739 --> 00:37:53,320
jiném modelu jako třeba DeepSeq nebo FII4

604
00:37:53,380 --> 00:37:55,539
nebo jakýkoliv další jiný modely,

605
00:37:55,779 --> 00:38:01,619
tak by to znamenalo, že vlastně ten samej dataset budu mít v těch čtyřech nebo pěti formátech,

606
00:38:01,699 --> 00:38:04,720
každej speciálně vytvořený pro ten daný model.

607
00:38:04,980 --> 00:38:10,260
To znamená, že se mi dá víc logický si udržet ten dataset v tom obecným formátu

608
00:38:10,639 --> 00:38:17,119
a pak využít schopnosti toho, co mi Hugging Face nabízí v těch svejch knihovnách,

609
00:38:17,260 --> 00:38:20,940
jako je právě ten Auto Tokenizer nebo Auto Class

610
00:38:21,340 --> 00:38:24,119
a ten už si to převede sám do těchto formatů.

611
00:38:27,420 --> 00:38:31,340
Takže je to čistě ten důvod, je čistě ten, že pro mě je jednodušší,

612
00:38:31,380 --> 00:38:35,300
že to drže v tom obecném formátu, který pak můžu transformovat

613
00:38:35,300 --> 00:38:38,000
do těch pěti různých verzích pro každej ten model,

614
00:38:38,000 --> 00:38:40,539
v závislosti na tom, který zrovna chci fajntunovat,

615
00:38:40,840 --> 00:38:44,000
než mít pět datasetů.

616
00:38:45,519 --> 00:38:50,279
Asi chápete, že když pak přidám nějaké údaje do toho obecního formátu,

617
00:38:50,380 --> 00:38:54,599
tak najednou se mi obohatí tím pádem ten fine tuning všech těch ostatních modelů.

618
00:38:54,860 --> 00:39:00,639
Když bych chtěl přidat pět, deset nebo tisíc zpráv do toho datasetu,

619
00:39:00,960 --> 00:39:05,659
tak bych musel pak obohatit nebo update-nout všech těch pět,

620
00:39:05,659 --> 00:39:07,420
v závislosti na tom, pro jaký model to je.

621
00:39:09,579 --> 00:39:19,320
Tak to by byly datasety, pojďme si ještě ukázat kód pro ten náš custom dataset.

622
00:39:20,440 --> 00:39:27,159
Tak já si tady ještě přeji to k trénování pomocí LLM, když ale chci vytvořit seznam čehokoliv od LLM,

623
00:39:27,220 --> 00:39:32,440
tak mi dá celkem málo příkladů. Ano, to je pravda, musím repetitivně volat.

624
00:39:32,900 --> 00:39:35,920
A teďko ne, já jsem ukazoval, že když jsme si připravovali ty data,

625
00:39:36,320 --> 00:39:43,639
Já jsem tady měl ten tool selection, tak jsem tady vlastně vám ukázal, jak se ty syntetické data vytvořit.

626
00:39:43,840 --> 00:39:48,320
A tady, pokud si to pamatuju správně, tak v tomhle tom skriptu bylo,

627
00:39:48,739 --> 00:39:56,300
že jsem volal repetitivně ve for each cyklu to API několikrát a omy, omy, omy.

628
00:39:57,360 --> 00:40:04,940
vytvářel, on mi vytvářel, jakoby, nakonec jsem byl schopný poskládat ten dataset poměrně velkej,

629
00:40:05,039 --> 00:40:10,199
ale tím, že jsem volal ten model, prostě třeba 100x nebo 20x,

630
00:40:10,400 --> 00:40:14,000
tak každej ten response mi vrátil třeba 10 exáplů.

631
00:40:15,920 --> 00:40:19,599
Tak ty malice ukažeme i kolem 100 000 září, jak bych na to měl jít prakticky,

632
00:40:19,599 --> 00:40:23,579
jestli chci vygramovat dataset pomocí LLM, tak to byla ta odpověď, super.

633
00:40:24,019 --> 00:40:28,159
Teď si ukážeme teda, jak tam uploadnout ty naše datasety.

634
00:40:28,420 --> 00:40:34,279
Jediný rozdíl, který v tom bude, nebo který si ukážeme,

635
00:40:35,220 --> 00:40:37,900
je jak naloudovat vlastně ten JSON.

636
00:40:38,159 --> 00:40:43,940
Takže já když to tady odevřu vedle sebe a ukázali bychom si tady ten Open Dataset,

637
00:40:44,039 --> 00:40:48,000
Open Assistant Dataset, tak tady vidíte, jak se ho naloudujete z toho hubu,

638
00:40:48,000 --> 00:40:49,300
kdy mu jenom dáte to jméno.

639
00:40:50,000 --> 00:40:53,159
To vám nebude fungovat pro ten váš lokální dataset,

640
00:40:53,260 --> 00:40:55,239
protože ten tam neexistuje.

641
00:40:55,279 --> 00:40:57,519
Zatím jsme se neukázali, jak ho tam pušnout.

642
00:40:57,900 --> 00:41:01,539
Asi tam bude i nějaký tlačítko upload,

643
00:41:01,539 --> 00:41:06,460
takže ručně byste teoreticky asi dokázali,

644
00:41:07,019 --> 00:41:08,380
to teda uvěřím tady,

645
00:41:09,059 --> 00:41:12,940
tam ten dataset na uploadovat.

646
00:41:15,179 --> 00:41:18,159
Nevidím, nevidím, nevidím format.

647
00:41:18,519 --> 00:41:20,179
Zkusím mít na sebe.

648
00:41:24,239 --> 00:41:25,320
Tak, dataset...

649
00:41:27,079 --> 00:41:32,119
Ani tady nevidím žádný tlačítko.

650
00:41:33,659 --> 00:41:39,420
Možná ho přehlížím, ale stejně prakticky byste to používali kódem.

651
00:41:40,619 --> 00:41:53,920
Vy by si první naloudujete ten dataset, stejně jako kdyby jsi ho budete stahovat z toho hubu, tak tady akorát řeknete, že chcete JSON format, nebo že to je JSON format a tady, kde je ten soubor.

652
00:41:53,940 --> 00:42:01,720
Takže tady vidíte, že tadyhle v tom data, v té složce mám ten all.json, který je úplně stejný, akorát jsem ho skopíroval.

653
00:42:02,480 --> 00:42:20,760
Když jsem tady měl pro to AI person, pro Joa a měli jsme tady v tom assembly.ai a byl tady ten open.ai format a byl tady ten hugging face format, tak to je ten JSON file, který já jsem prostě jen takhle vzal, udělal jsem copy a pastnul jsem si ho tady do toho data, do té data složky.

654
00:42:21,239 --> 00:42:24,340
Takže teď mu vlastně říkám, hele, nalouduj si ho,

655
00:42:24,340 --> 00:42:29,059
on si ho správně naformátuje do toho dataset formátu,

656
00:42:29,480 --> 00:42:36,500
že když se teď na to podíváme, tak uvidíte, jak je to vlastně naformátovaný,

657
00:42:36,579 --> 00:42:41,760
to znamená, kolik to má těch sloupečků, jestli to má tu trénovací sadu,

658
00:42:42,099 --> 00:42:44,599
sync, rerun.

659
00:42:45,599 --> 00:43:09,739
A teď se podíváme na ten soubor jako takovej, tak si pamatujete, že to je pole, hodnot, tady vlastně každej ten objekt je jeden ten

660
00:43:10,220 --> 00:43:12,380
podcast, teď v tomto případě,

661
00:43:12,519 --> 00:43:14,260
protože už to není ten

662
00:43:14,380 --> 00:43:16,559
OpenAI format, kde jsme museli

663
00:43:16,699 --> 00:43:18,920
odházet vlastně ty nevyhovující

664
00:43:19,059 --> 00:43:20,279
podcasty, který tam měly

665
00:43:20,380 --> 00:43:22,760
nějaký sexuální content nebo

666
00:43:22,900 --> 00:43:24,239
hate speech a tak dále,

667
00:43:24,239 --> 00:43:26,059
tak v tom Hugging Face formatu tam

668
00:43:26,059 --> 00:43:28,019
nás nikdo nějak neomezuje,

669
00:43:28,159 --> 00:43:30,000
takže tam máme všechny ty podcasty,

670
00:43:30,000 --> 00:43:31,559
co objekt, to podcast

671
00:43:32,260 --> 00:43:34,420
a teď každej ten podcast má vlastně

672
00:43:34,420 --> 00:43:36,079
tu propertu text,

673
00:43:36,079 --> 00:43:38,139
neboli ten sloupeček text.

674
00:43:38,000 --> 00:43:49,400
A do toho sloupečku já, jak vidíte, házím to pole, který obsahuje všechny tu konverzaci toho podcastu.

675
00:43:50,440 --> 00:43:56,019
Tak, u toho zůstanu teď a ukážu vám změnu, až se dostaneme k těm tools new.

676
00:43:56,500 --> 00:44:00,699
Takže teď si ukážeme, jak je to naloudovaný.

677
00:44:00,699 --> 00:44:05,980
To znamená, vidíte, že on mi vytvořil tu trénovací jedinej, tu propertu automaticky

678
00:44:06,139 --> 00:44:12,079
a všechny ty data z těch podkástů mi naházel do té trénovacího datasetu.

679
00:44:12,079 --> 00:44:14,960
Ten dataset má teda jenom jeden sloupeček, to je ten text.

680
00:44:15,599 --> 00:44:18,539
To je jediná properta, která je v tom objektu.

681
00:44:18,539 --> 00:44:20,920
Kdybych tady měl další, tak je to jenom další sloupeček.

682
00:44:21,559 --> 00:44:26,199
a ten text má počet řádek 31,

683
00:44:26,300 --> 00:44:31,480
neboli je tady 31 objektů, 31 podcastů.

684
00:44:31,480 --> 00:44:34,659
Ono reprezentuje toto počet těchto objektů,

685
00:44:34,659 --> 00:44:36,779
reprezentuje to počet podcastů.

686
00:44:37,699 --> 00:44:42,400
Tak a teď jsem tady ještě vyprintil takhle dílku,

687
00:44:42,779 --> 00:44:45,360
každého toho podcastu.

688
00:44:45,360 --> 00:44:47,260
To znamená, že je tady metoda

689
00:44:47,380 --> 00:44:48,619
CalculateLength.

690
00:44:48,960 --> 00:44:51,360
Vy v tom objektu datasetu jste schopni

691
00:44:51,440 --> 00:44:53,559
používat něco jako map funkci.

692
00:44:53,559 --> 00:44:54,980
Map funkce nedělá nic jiného,

693
00:44:54,980 --> 00:44:57,039
než pro každej ten řádek aplikuje

694
00:44:57,099 --> 00:44:57,940
tuto metodu.

695
00:44:58,559 --> 00:45:02,159
Do toho půjde ten jeden řádek.

696
00:45:02,159 --> 00:45:03,360
Z toho každého řádku já si

697
00:45:03,500 --> 00:45:04,539
vytáhnu ten text.

698
00:45:05,059 --> 00:45:08,920
a že tam jsou na dýlku toho pole, co je v tom textu,

699
00:45:08,920 --> 00:45:11,079
takže vlastně tohodle toho pole.

700
00:45:11,079 --> 00:45:15,320
To si uložím, přidám to, obohatím ten řádek

701
00:45:15,800 --> 00:45:18,960
v novou propertu Length,

702
00:45:19,159 --> 00:45:21,980
takže najednou ten každý řádek bude mít dvě property,

703
00:45:21,980 --> 00:45:24,279
nejenom text, ale bude to mít i Length.

704
00:45:24,300 --> 00:45:27,460
Takže ten Length mi bude říkat jenom, jak je to dlouhý,

705
00:45:27,460 --> 00:45:31,039
to jenom rozebírám pro toho, komu to není zřejmé,

706
00:45:31,039 --> 00:45:32,159
tadyhle z toho kusu kódu.

707
00:45:32,599 --> 00:45:39,460
A pak si ten dataset vyprintím si z té trénovací sady,

708
00:45:40,460 --> 00:45:44,300
pět záznamů a vemu si jenom tu Length propertu.

709
00:45:44,420 --> 00:45:49,239
Takže tím vám chci ukázat tu rozpříštěnost dat v podkástech.

710
00:45:49,239 --> 00:45:55,079
To znamená, že v prvním podkástu je 940 konverzací neboli těch výměn mezi User Assistant,

711
00:45:55,920 --> 00:45:58,360
pak i 661, 373,

712
00:45:58,360 --> 00:46:00,079
996, 707.

713
00:46:00,380 --> 00:46:01,760
To je něco, co my budeme muset

714
00:46:01,840 --> 00:46:03,659
v rámci toho datazetu řešit, až

715
00:46:03,800 --> 00:46:05,800
budeme řešit fine tuning, to si ukážeme

716
00:46:06,139 --> 00:46:07,440
v příští hodině.

717
00:46:07,460 --> 00:46:10,000
Teď je jenom důležité, abyste si

718
00:46:10,079 --> 00:46:11,340
uvědomili, že to má

719
00:46:11,739 --> 00:46:14,199
rozdílnou délku

720
00:46:14,320 --> 00:46:15,940
vlastně toho

721
00:46:16,159 --> 00:46:18,059
rozhovoru neboli těch výměn.

722
00:46:19,639 --> 00:46:20,720
Nejsou vždycky úplně

723
00:46:20,980 --> 00:46:23,800
stejné ty podcasty, to je asi jasný.

724
00:46:25,579 --> 00:46:27,099
Tak a teďka

725
00:46:28,039 --> 00:46:31,300
Já teď ještě ukazuji, jak vlastně tu dýdlku změnit

726
00:46:31,340 --> 00:46:33,960
nebo jak vlastně je první způsob,

727
00:46:34,220 --> 00:46:36,900
protože u neuronových sítí víte,

728
00:46:36,900 --> 00:46:38,880
že to funguje na maticovém principu.

729
00:46:38,880 --> 00:46:42,139
Matice vždycky má ty řádky stejně dlouhé,

730
00:46:42,139 --> 00:46:43,320
to jsme se říkali i minule.

731
00:46:43,619 --> 00:46:45,099
Takže máte určitý způsob,

732
00:46:45,119 --> 00:46:48,220
jakým způsobem tohohle dosáhnout,

733
00:46:48,220 --> 00:46:51,380
když pro nás teď je to v nevyhovujícím formátu

734
00:46:51,460 --> 00:46:54,420
a má to rozdílné délky těch řádků

735
00:46:54,619 --> 00:46:57,519
a ten úplně nejprimitivnější, který vás asi napadne,

736
00:46:57,519 --> 00:46:58,360
je, že to useknete.

737
00:46:59,220 --> 00:47:07,220
Takže tady je jenom jako, že změním tu dílku toho a useknu si to a vemu si jenom prostě prvních třeba deset jenom těch výměn.

738
00:47:07,220 --> 00:47:12,420
V tu chvíli má prostě každej ten podcast, těch 30 dyn na podcastu jenom deset těch výměn.

739
00:47:12,559 --> 00:47:20,840
No tak to je první taková jako strategie, je to zakomentovaný, my si k tomu ještě řekneme víc v příští hodině, takže není to důležitý.

740
00:47:21,119 --> 00:47:22,960
Jenom bych vám řekl, proč je to tady.

741
00:47:23,099 --> 00:47:28,739
A tady je poměrně pro někoho, kdo pracuje s Pandas knihovnou,

742
00:47:28,800 --> 00:47:35,300
tak jenom jak vlastně takovej dataset převést na Pandas format.

743
00:47:36,079 --> 00:47:38,760
Takže já to tady zase pustím, abyste to viděli.

744
00:47:39,380 --> 00:47:40,519
Hodně data...

745
00:47:41,840 --> 00:47:50,880
Data specialistů se mi nechcete říkat úplně, ale data inženýrů nebo lidí, co pracuje prostě s datama,

746
00:47:51,460 --> 00:47:56,140
tak v Pythonu konkrétně, tak používají Pandas, protože to usnadňuje tu práci,

747
00:47:56,440 --> 00:48:03,319
protože tady jenom pak vidíte, tady je vyprintěný vlastně ten dataset v tom Pandas formátu,

748
00:48:03,539 --> 00:48:06,559
takže tam už s tím můžete pracovat úplně stejně.

749
00:48:07,579 --> 00:48:24,940
Tak to je teda u všech těch datasetů, který tady máme, nebo kterými budeme používat, tak vlastně tyhle ty informace jsou tam stejný a na konci máte ukázaný, jak se to pušne do toho hubu, takže úplně takhle primitivně já to jdám prostě.

750
00:48:25,380 --> 00:48:27,059
2, jo.

751
00:48:27,400 --> 00:48:29,220
Takže vlastně si ho, tady to můžu

752
00:48:29,220 --> 00:48:31,400
zakomentovat, mě to teď nezajímá

753
00:48:31,400 --> 00:48:33,380
vůbec, takže výsledku

754
00:48:33,500 --> 00:48:35,380
si ho jenom naloudujete, převedete si to

755
00:48:35,380 --> 00:48:37,559
do toho formátu, do toho datasetu,

756
00:48:37,559 --> 00:48:39,779
který Hugging Face podporuje.

757
00:48:39,779 --> 00:48:41,340
Vy ho budete i používat, pokud

758
00:48:41,539 --> 00:48:43,099
budeme fine-tunovat

759
00:48:43,420 --> 00:48:45,400
a pak jenom na něm zavolám.

760
00:48:45,400 --> 00:48:49,579
Tak dobře, teď kompřeme,

761
00:48:49,579 --> 00:48:52,220
že jsem, jo, já jsem tady měl tuto

762
00:48:53,019 --> 00:48:53,599
takhle

763
00:48:55,720 --> 00:48:56,659
datasetu.

764
00:48:57,079 --> 00:48:59,760
Tak a půjším si to do toho hubu.

765
00:49:00,099 --> 00:49:02,260
Tento datasek, který jsem si lokálně

766
00:49:02,380 --> 00:49:04,739
naludoval. Takhle jednoduchý úplně to je.

767
00:49:05,380 --> 00:49:07,699
Pokud máte teda v environment variable

768
00:49:07,920 --> 00:49:09,619
dany ten váš klíč.

769
00:49:09,940 --> 00:49:11,739
Ten klíč je ve formátu HF token.

770
00:49:12,400 --> 00:49:18,140
Zase já si ten token promažu a tady je ten můj token, který tam je.

771
00:49:18,380 --> 00:49:22,920
Takže takovýhle soubor, když si vytvoří, tak on si automaticky ho z toho veme.

772
00:49:22,960 --> 00:49:26,960
Vidíte, že já mu tady ani neříkám, jestli existuje nebo neexistuje.

773
00:49:27,859 --> 00:49:32,920
A teď, když bych to zavolal, tak bych ho začal uploadovat.

774
00:49:35,440 --> 00:49:41,220
tak Creating Python Arrow Format a teď mi ho tam vlastně naházel,

775
00:49:41,340 --> 00:49:44,559
takže já si tady, když si tady dám ten Hugging Face zase,

776
00:49:45,880 --> 00:49:48,819
tak bych tady měl teď jet v tom mém,

777
00:49:51,319 --> 00:49:57,239
tady Dataset, já budu vzít tadyhle, kliknu tady a tady bude Dataset Joe Small 2.

778
00:49:58,380 --> 00:49:58,539
Jo?

779
00:49:58,539 --> 00:50:04,659
Tady vy pak vidíte teda kolik to má řádek, 31 řádek a tady pak vidíte všechny ty konverzace.

780
00:50:05,559 --> 00:50:08,699
Co je důležité u tohohle si uvědomit je,

781
00:50:08,779 --> 00:50:13,680
že já jsem tam vlastně nekomitnul stringovou reprezentaci každého toho podcastu.

782
00:50:13,680 --> 00:50:17,400
Vidíte, že tady je prostě list, takže když se to jmenuje text,

783
00:50:17,420 --> 00:50:21,300
nebo je to sloupeček text, tak je to vlastně obsahověto list objektů.

784
00:50:21,760 --> 00:50:28,140
Takže zase ta roztříštěnost těch formatů na to budete narážet.

785
00:50:28,659 --> 00:50:30,859
Nebojte se vůbec toho.

786
00:50:31,000 --> 00:50:34,279
Já bych preferoval vždycky tu objektovou reprezentaci,

787
00:50:34,279 --> 00:50:36,980
ne textovou, ale není to nic proti ničemu.

788
00:50:36,980 --> 00:50:39,180
Je to taková moje osobní preference.

789
00:50:39,599 --> 00:50:44,279
A pak tady vidíte, kolik vlastně těch výměn,

790
00:50:45,180 --> 00:50:50,380
proto zdůraznuju vlastně ten format toho Datasetu nebo toho sloupečku,
=================


Important: Translate all titles to English.
Important: No explanation, no comments, only valid JSON as output.